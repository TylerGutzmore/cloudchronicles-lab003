{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6f2eff9b",
      "metadata": {
        "id": "6f2eff9b"
      },
      "source": [
        "# üß† cloudChronicles Lab #003: Resilience-as-a-Service MVP\n",
        "\n",
        "**Lab Type:** MVP/Product  \n",
        "**Estimated Time:** 180‚Äì360 mins  \n",
        "**Skill Level:** Advanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d527eb1c",
      "metadata": {
        "id": "d527eb1c"
      },
      "outputs": [],
      "source": [
        "# Let's begin by printing your name to personalize the notebook\n",
        "your_name = \"\"\n",
        "print(f\"Welcome to the lab, {tyler_gutzmore}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1950fc89",
      "metadata": {
        "id": "1950fc89"
      },
      "source": [
        "## üîç STAR Method Lab Prompt\n",
        "\n",
        "**Situation:**  \n",
        "[Define your scenario here.]\n",
        "\n",
        "**Task:**  \n",
        "[Define what the user is expected to solve.]\n",
        "\n",
        "**Action:**  \n",
        "[Step-by-step instructions using GCP tools.]\n",
        "\n",
        "**Expected Result:**  \n",
        "[A defined deliverable such as a DR plan, diagram, MVP, etc.]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d06ec817",
      "metadata": {
        "id": "d06ec817"
      },
      "source": [
        "## ‚úçÔ∏è Your Assignment\n",
        "\n",
        "_Use this section to complete your deliverable:_\n",
        "\n",
        "```markdown\n",
        "(Example Format)\n",
        "\n",
        "- **MVP Name**: Resilience-as-a-Service  \n",
        "- **User Inputs**: Industry, Location, Cloud Stack  \n",
        "- **Output**: Custom DRP in PDF  \n",
        "- **Tech Stack**: Gemini API + Flask + Firebase  \n",
        "- **Hosting**: Deployed via Bluehost (cPanel) or Cloud Run  \n",
        "- **Screenshots/Links**: [Attach demo or screenshot]  \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e11ebd7"
      },
      "source": [
        "## üîç STAR Method Lab Prompt\n",
        "\n",
        "**Situation:**\n",
        "A critical simulated regional outage has occurred in Google Cloud's `us-central1` region, rendering many services within that region unavailable. This outage is impacting our primary production environment, which includes Compute Engine instances running our web servers and application logic, a crucial Cloud SQL database storing all our customer and transaction data, and Cloud Storage buckets used for storing static assets, backups, and user-uploaded content. The potential business impact is severe and immediate, leading to a complete inability for customers to access our services, a halt in all transaction processing, potential data inconsistencies if not handled properly, and significant financial losses due to downtime. Furthermore, the outage damages our reputation and erodes customer trust. We need a robust and well-defined plan to recover from this situation quickly and efficiently.\n",
        "\n",
        "**Task:**\n",
        "Our task is to develop a comprehensive, exceptionally detailed, and easily understandable disaster recovery plan specifically for a regional outage in Google Cloud's `us-central1`. This plan must leverage key Google Cloud disaster recovery tools such as Cloud SQL cross-region replicas, multi-region Cloud Storage, and Pub/Sub for alerting and automation. The plan should clearly articulate the failover and recovery processes in a step-by-step manner, written in language so simple and clear that someone with minimal technical background can comprehend and follow it. The goal is to ensure business continuity and minimize the impact of such an outage.\n",
        "\n",
        "**Action:**\n",
        "\n",
        "Here is our highly detailed, step-by-step disaster recovery plan to mitigate the impact of a `us-central1` regional outage:\n",
        "\n",
        "1.  **Cloud SQL Cross-Region Replicas: Our Database Safety Net:**\n",
        "    *   **Preparation is Key:** Before an outage even occurs, we have meticulously set up a cross-region read replica of our primary Cloud SQL instance, which resides in `us-central1`. This replica is located in a geographically distinct and independent region, such as `us-east4`. Think of this replica as a constantly updated backup copy of our entire database, kept safe in a different location. Data changes from the primary instance in `us-central1` are continuously and automatically sent to the replica in `us-east4` using asynchronous replication. This means the replica is usually only a few seconds behind the primary, minimizing potential data loss during a failover.\n",
        "    *   **Constant Monitoring:** We employ Google Cloud's Cloud Monitoring to keep a watchful eye on the health and performance of our primary Cloud SQL instance. We have configured specific alert policies that trigger when certain critical conditions are met, such as the instance becoming unreachable, experiencing extremely high error rates, or reporting replication lag exceeding a predefined threshold.\n",
        "    *   **Alerting via Pub/Sub:** When a monitoring alert is triggered, it automatically publishes a message to a dedicated Pub/Sub topic we've created specifically for disaster recovery alerts, named `dr-outage-alerts`. Pub/Sub acts like a central message hub, allowing different parts of our system to react to the outage notification. The message contains details about the specific alert and the affected resource.\n",
        "    *   **Automated Failover Trigger:** A Cloud Function, a small piece of code that runs in response to events, is subscribed to the `dr-outage-alerts` Pub/Sub topic. When it receives a message indicating a critical issue with the primary Cloud SQL instance (like an outage), this function is automatically executed. Its primary role is to initiate the failover process for the database.\n",
        "    *   **Promoting the Replica:** The Cloud Function, using the Google Cloud client libraries or gcloud commands, sends a command to Google Cloud to \"promote\" the read replica in `us-east4`. This promotion process converts the read-only replica into a fully functional, independent primary instance. Google Cloud handles the necessary steps to make this instance writable and ready to serve application traffic. This process typically takes a few minutes to complete.\n",
        "    *   **Application Redirection:** Our applications don't connect directly to the IP address of the database instance. Instead, they use a database connection string that points to a DNS record managed by Cloud DNS. This DNS record initially points to the primary instance in `us-central1`. As part of the automated failover triggered by the Cloud Function, the Cloud DNS record is updated to point to the IP address of the newly promoted primary instance in `us-east4`. Because applications resolve the DNS name, they will automatically start connecting to the database in the disaster recovery region without requiring code changes or manual configuration updates within the application itself.\n",
        "\n",
        "2.  **Multi-Region Cloud Storage: Ensuring Data is Always There:**\n",
        "    *   **Beyond a Single Location:** Instead of using regional Cloud Storage buckets, which store data in a single Google Cloud region, we utilize multi-region buckets for all our critical data, including website assets, user uploads, and backups.\n",
        "    *   **Automatic Replication:** When you upload data to a multi-region bucket, Google Cloud automatically and asynchronously replicates that data to at least two geographically separate regions within the chosen multi-region configuration (e.g., the `US` multi-region includes locations like `us-central1`, `us-east4`, `us-west1`, etc.). This built-in replication is handled entirely by Google Cloud infrastructure.\n",
        "    *   **Seamless Access During Outage:** If the `us-central1` region becomes unavailable, applications attempting to access data from our multi-region buckets will be automatically and transparently served the data from an available replica in another region within the multi-region configuration. The application doesn't need to know which region is serving the data; Google Cloud handles the routing. This ensures that our website assets load, user files are accessible, and backups can be retrieved even if one region is completely down.\n",
        "    *   **Data Consistency and Versioning:** Multi-region buckets provide strong global consistency for objects, meaning that once an object is written, any subsequent read request from anywhere in the world will see the latest version. We also enable object versioning on our buckets. This feature keeps previous versions of an object whenever it's overwritten or deleted, providing an additional layer of protection against accidental data loss or malicious activity.\n",
        "\n",
        "3.  **Pub/Sub Alerts and Intelligent Automation:**\n",
        "    *   **The Central Nervous System:** Pub/Sub acts as the central nervous system for our automated disaster recovery process. It allows different components of our system to communicate and react to events in a decoupled manner.\n",
        "    *   **Comprehensive Monitoring Integration:** Cloud Monitoring is configured with a wide array of alert policies that go beyond just the database. We monitor the health and availability of our Compute Engine instances, GKE clusters, load balancers, and other critical services in `us-central1`. Alerts for these services are also published to the `dr-outage-alerts` Pub/Sub topic.\n",
        "    *   **Triggering Automated Actions:** Multiple Cloud Functions are subscribed to the `dr-outage-alerts` topic, each designed to perform a specific recovery action based on the content of the alert message:\n",
        "        *   A function specifically listens for database alerts and triggers the Cloud SQL replica promotion (as described in step 1).\n",
        "        *   Another function listens for Compute Engine or GKE alerts. Upon receiving such an alert related to `us-central1`, it initiates the deployment of our application stack in the `us-east4` region. This might involve creating new VM instances from pre-configured machine images and instance templates, or deploying our application containers to the GKE cluster in `us-east4` using kubectl commands or deployment pipelines.\n",
        "        *   A third function is responsible for updating our global HTTP(S) Load Balancer. When the `us-central1` backend becomes unhealthy due to the outage, this function programmatically removes the `us-central1` backend from the load balancer's configuration and ensures that all incoming traffic is directed to the healthy backend in `us-east4`. This is a critical step to quickly redirect user traffic to the operational environment.\n",
        "    *   **Customizable Responses:** By using Pub/Sub and Cloud Functions, we can easily add or modify automated recovery actions in the future without affecting other parts of the system. This makes our disaster recovery plan flexible and adaptable.\n",
        "\n",
        "4.  **Application Failover Procedures: Getting Applications Running Elsewhere:**\n",
        "    *   **Compute Engine Strategy:** For applications running on Compute Engine, our failover strategy involves having up-to-date custom machine images of our configured application servers. These images serve as blueprints for quickly creating new, identically configured VMs. The automated Cloud Function triggered by the Pub/Sub alert uses these images and pre-defined instance templates (which specify machine types, network configurations, and startup scripts) to rapidly provision new VM instances in the `us-east4` region. Our global HTTP(S) Load Balancer, already configured with backends in both `us-central1` and `us-east4`, automatically detects the unhealthiness of the `us-central1` instances and directs all new incoming traffic to the healthy instances in `us-east4`. We also manage our external IP addresses carefully, potentially using global static external IP addresses that can be reassigned to instances in the disaster recovery region.\n",
        "    *   **Google Kubernetes Engine Strategy:** For containerized applications managed by GKE, we maintain a secondary, smaller GKE cluster in the `us-east4` region. Our CI/CD pipelines and deployment manifests are designed to be easily deployable to this secondary cluster. In the event of an outage, the automated Pub/Sub triggered Cloud Function or a manual trigger initiates the deployment of our application's containers to the `us-east4` GKE cluster. We utilize either GKE's multi-cluster services or a global load balancer in front of both clusters to manage traffic routing. Persistent storage for stateful applications is handled using regional persistent disks within `us-east4` that are created during the failover process and attached to the new pods.\n",
        "    *   **Other Services:** For any other Google Cloud services our application relies on, we have documented their specific disaster recovery procedures. This might involve configuring regional or multi-regional settings, utilizing backup and restore mechanisms, or having standby instances in the disaster recovery region.\n",
        "\n",
        "5.  **Monitoring the Recovery and Planning the Failback:**\n",
        "    *   **Vigilant Monitoring in DR Region:** Once the failover is complete and our applications are running in `us-east4`, we intensify our monitoring efforts in this region. We use Cloud Monitoring dashboards and alerts to track the health, performance, and capacity utilization of all our resources in `us-east4`. We ensure that application logs are being collected and analyzed in Cloud Logging to quickly identify and troubleshoot any issues in the disaster recovery environment.\n",
        "    *   **Monitoring Original Region Recovery:** Simultaneously, we closely monitor the status of the `us-central1` region. We rely on the official Google Cloud status dashboard for updates on the regional outage. We also have monitoring checks in place to determine when our infrastructure and services in `us-central1` become available and stable again.\n",
        "    *   **The Failback Strategy:** Once the `us-central1` region has fully recovered and is confirmed to be stable and operational, we will initiate the failback process. This is often a more complex and carefully managed process than failover to minimize disruption.\n",
        "        *   **Data Synchronization:** A critical step is ensuring data consistency between the primary database (currently in `us-east4`) and the recovering instance in `us-central1`. This might involve setting up replication *back* to `us-central1` from `us-east4` and waiting for the instances to fully synchronize.\n",
        "        *   **Gradual Traffic Shift:** We will gradually shift traffic back to the services in `us-central1`. This is done by updating the global load balancer configuration to slowly reintroduce the `us-central1` backend and direct a small percentage of traffic to it initially. We monitor closely for any errors or performance issues before increasing the traffic percentage.\n",
        "        *   **Database Role Reversal:** Once data is synchronized and we are confident in the stability of `us-central1`, we will perform a controlled failback of the database. This might involve promoting the instance in `us-central1` back to primary and configuring the instance in `us-east4` as a replica again. This step requires careful planning and potentially a brief maintenance window.\n",
        "        *   **Resource Cleanup:** After successfully failing back to `us-central1`, we will scale down or terminate the resources (VMs, GKE clusters, etc.) that were spun up in `us-east4` for disaster recovery purposes to avoid incurring unnecessary costs.\n",
        "\n",
        "6.  **Regular Testing and Living Documentation: Staying Prepared:**\n",
        "    *   **Practice Makes Perfect:** A disaster recovery plan is only effective if it works when you need it most. Therefore, we conduct regular disaster recovery drills and simulations at least twice a year. These aren't just theoretical exercises. We perform tabletop exercises where the team walks through the steps of the plan verbally, identifying any ambiguities or missing information. More importantly, we conduct simulated failovers in a non-production or staging environment that closely mirrors our production setup. These simulations validate our automated processes, test the manual steps, and identify any bottlenecks or points of failure in the plan.\n",
        "    *   **Keeping it Current:** Technology evolves, and our infrastructure changes. It is absolutely critical that our disaster recovery documentation is always up-to-date. Any changes to our application architecture, database configuration, or infrastructure setup must be reflected in the disaster recovery plan documentation.\n",
        "    *   **Accessibility is Key:** The detailed documentation of this disaster recovery plan, including contact lists of key personnel, roles and responsibilities during an incident, step-by-step procedures, and configuration details, is stored in a highly available and easily accessible location. This includes storing copies in multiple cloud storage locations (potentially even outside of Google Cloud for extreme scenarios) and ensuring key personnel have offline access to the documentation.\n",
        "\n",
        "**Expected Result:**\n",
        "The successful outcome of this plan is a rigorously tested, well-documented, and highly resilient application architecture on Google Cloud. The plan, leveraging the power of Cloud SQL cross-region replicas for database failover, multi-region Cloud Storage for continuous data availability, and Pub/Sub for intelligent alerting and automation, ensures that in the event of a regional outage in `us-central1`, our services can be quickly and efficiently failed over to the `us-east4` region with minimal data loss and downtime. The expected result is not just a document, but a proven capability to maintain business continuity, protect our data, and quickly restore full operations, thereby preserving customer trust and minimizing financial impact."
      ],
      "id": "9e11ebd7"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}